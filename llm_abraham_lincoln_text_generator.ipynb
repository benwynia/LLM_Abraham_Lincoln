{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a396eb4",
   "metadata": {},
   "source": [
    "Generatively Pretrained Transformer (GPT)\n",
    "https://arxiv.org/pdf/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae9a853",
   "metadata": {},
   "source": [
    "<h3> 1. Import Libraries </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd1f6530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.10\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc4931eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All specified modules loaded\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# list of modules to install and their corresponding names\n",
    "modules = {'tensorflow': 'tf',\n",
    "           'numpy': 'np',\n",
    "           'pandas': 'pd',\n",
    "           'requests': 'requests',\n",
    "           'tiktoken': 'tiktoken',\n",
    "           'openai': 'openai',\n",
    "           'torch': 'torch',\n",
    "           'torch.nn': 'nn',\n",
    "           'matplotlib.pyplot': 'plt',\n",
    "           'torch.nn.functional': 'F',\n",
    "           'bs4': 'bs4',\n",
    "           're': 're',\n",
    "           'os': 'os',\n",
    "          'json': 'json'}\n",
    "\n",
    "# iterate over the modules and install them if necessary\n",
    "for module, name in modules.items():\n",
    "    try:\n",
    "        exec(f\"import {module} as {name}\")\n",
    "    except ModuleNotFoundError as error:\n",
    "        print(f\"{error.name} module not found. Installing {error.name}...\")\n",
    "        try:\n",
    "            subprocess.check_call([\"pip\", \"install\", error.name])\n",
    "        except:\n",
    "            print(f\"Installation of {module} failed.\")\n",
    "    else:\n",
    "        continue\n",
    "print(\"All specified modules loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7be65d0",
   "metadata": {},
   "source": [
    "<b> Check for available GPUs for Training </b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e07794d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<torch.cuda.device at 0x7fb11839f730>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for GPUs\n",
    "available_gpus = [torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "available_gpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2a9b97",
   "metadata": {},
   "source": [
    "<h3> 2. Import Data </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16e87f43-c2ff-4a1a-b66b-c7ad494901c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_html(url):\n",
    "    response = requests.get(url)\n",
    "    html_content = response.content\n",
    "    return html_content\n",
    "\n",
    "def remove_special_chars(text):\n",
    "    return re.sub(r'\\xa0', '', text)\n",
    "\n",
    "def process_html(html_content):\n",
    "    soup = bs4.BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove page number expressions\n",
    "    for pbtext in soup.find_all('span', class_='pbtext'):\n",
    "        pbtext.decompose()\n",
    "\n",
    "    entries = []\n",
    "    \n",
    "    for h2 in soup.find_all('h2'):\n",
    "        title = remove_special_chars(h2.get_text(strip=True))\n",
    "        \n",
    "        text_div = h2.find_next('div', attrs={'class': 'textindentlevelx'})\n",
    "        opener_div = text_div.find('div', attrs={'class': 'opener'})\n",
    "        opener = remove_special_chars(opener_div.get_text(strip=True, separator=' ')) if opener_div else ''\n",
    "        text = remove_special_chars(' '.join(p.get_text(strip=True, separator=' ') for p in text_div.find_all('p')))\n",
    "\n",
    "        annotation_div = text_div.find_next_sibling('div', attrs={'class': 'textindentlevelx'})\n",
    "        annotation = remove_special_chars(' '.join(p.get_text(strip=True, separator=' ') for p in annotation_div.find_all('p')))\n",
    "        \n",
    "        entry = {\n",
    "            'title': title,\n",
    "            'opener': opener,\n",
    "            'text': text,\n",
    "            'annotation': annotation\n",
    "        }\n",
    "        entries.append(entry)\n",
    "    \n",
    "    return entries\n",
    "\n",
    "def remove_bracketed_numbers(text):\n",
    "    return re.sub(r'\\[\\d+\\]', '', text)\n",
    "\n",
    "def create_text_file(entries, output_filename):\n",
    "    full_text = ''\n",
    "    with open(output_filename, 'w', encoding='utf-8') as f:\n",
    "        for entry in entries:\n",
    "            title = remove_bracketed_numbers(entry['title'])\n",
    "            opener = remove_bracketed_numbers(entry['opener'])\n",
    "            text = remove_bracketed_numbers(entry['text'])\n",
    "\n",
    "            entry_text = f\"{title}\\n{opener}\\n{text}\\n\\n\"\n",
    "            full_text += entry_text\n",
    "            f.write(entry_text)\n",
    "    \n",
    "    return full_text\n",
    "\n",
    "def get_texts(url, filename):\n",
    "    html_content = fetch_html(url)\n",
    "    entries = process_html(html_content)\n",
    "    full_text = create_text_file(entries, filename)\n",
    "\n",
    "    return html_content, entries, full_text\n",
    "\n",
    "def process_volumes():\n",
    "    for i in range(1, 9):\n",
    "        filename = f'lincoln_volume_{i}.txt'\n",
    "\n",
    "        if os.path.isfile(filename):\n",
    "            print(f'File for {filename} already exists. Skipping.')\n",
    "        else:\n",
    "            link = f'https://quod.lib.umich.edu/l/lincoln/lincoln{i}?rgn=main;view=fulltext'\n",
    "            html_content, entries, full_text = get_texts(link, filename)\n",
    "            print(f'Files for {filename} written to disk.')\n",
    "\n",
    "            # Store objects in memory with different names\n",
    "            globals()[f'l{i}_html_content'] = html_content\n",
    "            globals()[f'l{i}_entries'] = entries\n",
    "            globals()[f'l{i}_full_text'] = full_text\n",
    "            \n",
    "            # Save HTML and JSON files with correct naming format\n",
    "            html_filename = f'lincoln_v{i}_webpage.html'\n",
    "            json_filename = f'lincoln_v{i}_entries.json'\n",
    "            \n",
    "            with open(html_filename, 'w', encoding='utf-8') as f:\n",
    "                f.write(html_content.decode())\n",
    "            \n",
    "            with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(entries, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c506731b-20b0-4fd7-9347-c60c66cf10c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files for lincoln_volume_1.txt written to disk.\n",
      "Files for lincoln_volume_2.txt written to disk.\n",
      "Files for lincoln_volume_3.txt written to disk.\n",
      "Files for lincoln_volume_4.txt written to disk.\n",
      "Files for lincoln_volume_5.txt written to disk.\n",
      "Files for lincoln_volume_6.txt written to disk.\n",
      "Files for lincoln_volume_7.txt written to disk.\n",
      "Files for lincoln_volume_8.txt written to disk.\n"
     ]
    }
   ],
   "source": [
    "process_volumes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3266866-e17c-4780-a135-c0fe17b6670f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_text_files(filenames):\n",
    "    long_text = ''\n",
    "    for filename in filenames:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            file_text = f.read()\n",
    "            long_text += file_text\n",
    "    return long_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de26e976-e655-463f-95fa-8075e3d78a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = ['lincoln_volume_1.txt',\n",
    "             'lincoln_volume_2.txt',\n",
    "             'lincoln_volume_3.txt',\n",
    "             'lincoln_volume_4.txt',\n",
    "             'lincoln_volume_5.txt',\n",
    "             'lincoln_volume_6.txt',\n",
    "             'lincoln_volume_7.txt',\n",
    "             'lincoln_volume_8.txt']\n",
    "\n",
    "text = concatenate_text_files(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc1510c",
   "metadata": {},
   "source": [
    "<h3> 3. Basic Data Exploration </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461987b9",
   "metadata": {},
   "source": [
    "<h3> 4. Create Basic Tokenization Functions and Encode Data </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf04dc",
   "metadata": {},
   "source": [
    "<b> Encoding 2: Subword OpenAI GPT2 Dictionary</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bf6cf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the gpt2 encoding dictionary is 50257\n",
      "[15496, 2159]\n",
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "# open ai tokenizer function goes here (tiktoken)\n",
    "gpt2enc = tiktoken.get_encoding('gpt2')\n",
    "# gpt4enc = tiktoken.get_encoding('cl100k_base')\n",
    "vocab_size=gpt2enc.n_vocab\n",
    "print(f\"The length of the gpt2 encoding dictionary is {vocab_size}\")\n",
    "print(gpt2enc.encode(\"Hello World\"))\n",
    "print(gpt2enc.decode(gpt2enc.encode(\"Hello World\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6deb4486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence piece from google"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0683d0",
   "metadata": {},
   "source": [
    "<h3> 5. Encode Dataset </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba521886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1596331]) torch.int64\n",
      "tensor([29881,  2070, 18535,   274,   198,    58,  1507,  1731,    12,  1507,\n",
      "         2075,    60,   198,  4826, 13220, 12406,   465,  1021,   290,  3112,\n",
      "          339,   481,   307,   922,   475,  5770,  4206,  1649,   220, 16660,\n",
      "        12406,   465,  1021,   290,  3112,   339,   481,   307,   922,   475,\n",
      "         5770,  4206,  1649,  3862,  1867,   281,   795,   774,   410,  2136,\n",
      "          256,   271,   290,  1528,   703, 14622,   484,   389, 14622,   355,\n",
      "          281,   773,   666,  5240,    58,   322,    60, 46423,  6129,   319,\n",
      "          588,   257,  4395,  3491,   262,   906,   415,  2589,  2329,   685,\n",
      "          271,   994,    60,   788, 19392,  1497,   287,   289,    58,   292,\n",
      "           60,   660,   326,   356,   685,  5171,    60,  1239,   910,   484])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(gpt2enc.encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f1c0d5",
   "metadata": {},
   "source": [
    "<h3> 6. Create Training and Validation Datasets </h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f57e81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate_split(dataset,cutoff):\n",
    "    n = int(cutoff*len(dataset))\n",
    "    train_data = dataset[:n]\n",
    "    val_data = dataset[n:]\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00ad22a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 0.90\n",
    "train_data, val_data = train_validate_split(data,cutoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b9d40c",
   "metadata": {},
   "source": [
    "<h3> 7. Define Basic Bigram Transformer Model </h3> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31986745-d699-4774-8bc4-3e616a1cde37",
   "metadata": {},
   "source": [
    "A Bigram transformer is a type of language model that uses the concept of bigrams to predict the next word in a sequence of words. Bigrams are pairs of adjacent words in a sentence or text. For example, in the sentence \"The quick brown fox jumps over the lazy dog,\" the bigrams are \"The quick\", \"quick brown\", \"brown fox\", \"fox jumps\", \"jumps over\", \"over the\", and \"the lazy\".\n",
    "\n",
    "A Bigram transformer learns the probability distribution of bigrams from a given text corpus, and uses this knowledge to predict the most likely next word in a sentence based on the previous word. It is a simple and effective approach for language modeling that can be used for a variety of natural language processing tasks, such as machine translation, speech recognition, and text generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a0836-b100-47e2-a7f5-a2f13967c9ed",
   "metadata": {},
   "source": [
    "<p>This neural network is designed for text generation and is based on the architecture of the Transformer model, which is widely used in natural language processing tasks. The model processes text in the form of tokens and generates a probability distribution over the vocabulary for the next token. It takes into account the order of the tokens and can generate text by sampling from this distribution.</p>\n",
    "<p>The high-level organization of the model includes the following components:</p>\n",
    "<ul>\n",
    "    <li>Token and position embeddings: These are look-up tables that convert input token indices into continuous vectors. The token embeddings represent the meaning of each token, while the position embeddings capture the position of the tokens in the input sequence. These embeddings are combined to form the initial input representation.</li>\n",
    "    <li>Transformer blocks: These are the main building blocks of the model and consist of self-attention layers and feed-forward layers. There are 6 (n_layer) such blocks stacked on top of each other.</li>\n",
    "    <li>Self-attention layers: The model has multi-head self-attention layers, where each head computes attention scores between tokens and aggregates the information from different positions. There are 6 (n_head) heads in total, and each head has its own set of learnable parameters for computing key, query, and value vectors.</li>\n",
    "    <li>Feed-forward layers: These are simple fully connected layers with a ReLU activation function in between. They serve as an additional non-linear transformation within each Transformer block.</li>\n",
    "    <li>Layer normalization: This is applied after the self-attention and feed-forward layers in each block to stabilize training and improve convergence.</li>\n",
    "    <li>Final layer normalization and linear layer: After passing through all the Transformer blocks, the output goes through a final layer normalization followed by a linear layer. The linear layer's output has the same dimension as the vocabulary size, representing logits for each possible token.</li>\n",
    "    <li>Loss computation: If targets are provided, the model computes the cross-entropy loss between the logits and target tokens.</li>\n",
    "</ul>\n",
    "<p>The model's input layer is essentially the token and position embeddings, while the hidden layers are the Transformer blocks. There are 6 (n_layer) hidden layers in this model, each consisting of a self-attention layer and a feed-forward layer. The output layer is the final linear layer that produces logits for each token in the vocabulary.</p>\n",
    "<p>To generate text, the model takes an input sequence, processes it through the network, and samples a new token from the distribution of the last time step. This process is repeated for a specified number of new tokens to generate text conditioned on the input context.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0834b174-7b40-448c-bf1f-c646e3653f0c",
   "metadata": {},
   "source": [
    "<h4> A. Define Hyperparameters </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba4e84-f2a2-4812-a72b-b7475e4cc4a8",
   "metadata": {},
   "source": [
    "<ul>\n",
    "  <li><code>batch_size</code>: This hyperparameter refers to the number of independent sequences that the neural network will process in parallel during each training iteration. A higher batch size can speed up training but requires more memory.</li>\n",
    "  <li><code>block_size</code>: This hyperparameter specifies the maximum length of input sequences that the neural network can process at once. Any input sequence longer than this will be truncated or split into smaller blocks. The value of <code>block_size</code> affects the amount of context that the network can use to make predictions.</li>\n",
    "  <li><code>max_iters</code>: This hyperparameter specifies the maximum number of training iterations or epochs that the neural network will undergo during the training process.</li>\n",
    "  <li><code>eval_interval</code>: This hyperparameter specifies the frequency (in training iterations) at which the neural network's performance will be evaluated on a validation set.</li>\n",
    "  <li><code>learning_rate</code>: This hyperparameter controls the step size that the optimizer takes during backpropagation to update the weights of the neural network. A higher learning rate can result in faster convergence during training, but it may also cause the model to converge to a suboptimal solution.</li>\n",
    "  <li><code>device</code>: This hyperparameter specifies the device on which the neural network will be trained. It can be set to <code>'cuda'</code> if a GPU is available or <code>'cpu'</code> if not.</li>\n",
    "  <li><code>eval_iters</code>: This hyperparameter specifies the number of training iterations between each evaluation of the neural network's performance on a validation set.</li>\n",
    "  <li><code>n_embd</code>: This hyperparameter specifies the dimensionality of the embedding layer in the transformer network. The embedding layer maps each token in the input sequence to a high-dimensional vector representation.</li>\n",
    "  <li><code>n_head</code>: This hyperparameter specifies the number of attention heads in the transformer network. The attention mechanism allows the network to selectively attend to different parts of the input sequence.</li>\n",
    "  <li><code>n_layer</code>: This hyperparameter specifies the number of transformer layers in the network. Each transformer layer consists of a series of multi-head attention and feedforward layers.</li>\n",
    "  <li><code>dropout</code>: This hyperparameter specifies the probability of dropping out (i.e., setting to zero) each element of the input during training. Dropout is a regularization technique that helps prevent overfitting by forcing the network to learn more robust representations.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b8eba86f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fb120007cf0>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128 # how many independent sequences will we process in parallel?\n",
    "block_size = 64 # what is the maximum context length for predictions?\n",
    "max_iters = 3000\n",
    "eval_interval = 50\n",
    "learning_rate = 3e-4\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 1200\n",
    "n_head = 8\n",
    "n_layer = 8\n",
    "dropout = 0.2\n",
    "patience = 3\n",
    "plateau = 0.01\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b75e994",
   "metadata": {},
   "source": [
    "<h4> B. Data Batching Function </h4> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbf91db",
   "metadata": {},
   "source": [
    "This function is used for generating a mini-batch of data from the specified dataset (training or validation) to be used during the training or evaluation process of a machine learning model. It randomly selects batch_size starting indices, then creates input sequences (x) and their corresponding target sequences (y) of length block_size. The input and target sequences are shifted by one position, making the model predict the next element in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "daa16676",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(split):\n",
    "    \"\"\"\n",
    "    This function generates a small batch of data consisting of inputs (x) and targets (y) from the training or validation dataset.\n",
    "    \n",
    "    Args:\n",
    "    split (str): A string that indicates whether to use the training dataset or the validation dataset. It accepts two values, 'train' or 'val'.\n",
    "    \n",
    "    Returns:\n",
    "    x (torch.Tensor): A tensor containing the input sequences for the mini-batch. The shape of the tensor is (batch_size, block_size).\n",
    "    y (torch.Tensor): A tensor containing the target sequences for the mini-batch. The shape of the tensor is (batch_size, block_size).\n",
    "    \n",
    "    Global Variables:\n",
    "    train_data (torch.Tensor): A tensor containing the training dataset.\n",
    "    val_data (torch.Tensor): A tensor containing the validation dataset.\n",
    "    batch_size (int): The number of samples in a single batch.\n",
    "    block_size (int): The length of each input and target sequence.\n",
    "    \"\"\"\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252dca8a",
   "metadata": {},
   "source": [
    "<h4> C. Loss Estimation Function </h4> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6704c08-a280-46b9-816e-0e410beba0fc",
   "metadata": {},
   "source": [
    "<p><code>estimate_loss()</code></p>\n",
    "<strong>Input:</strong> \n",
    "<p>None</p>\n",
    "<strong>Output:</strong> \n",
    "<p>A Python dictionary containing the average loss over the training and validation datasets.</p>\n",
    "<strong>Functionality:</strong> \n",
    "<p>This function estimates the average loss over the training and validation datasets for a PyTorch model.</p>\n",
    "<strong>Steps:</strong> \n",
    "<ol>\n",
    "    <li>Initializes an empty Python dictionary <code>out</code> to store the average loss for each dataset.</li>\n",
    "    <li>Sets the model to evaluation mode using <code>model.eval()</code>.</li>\n",
    "    <li>Loops over the two splits of the dataset: training and validation.</li>\n",
    "    <li>Initializes a PyTorch tensor <code>losses</code> with shape <code>(eval_iters,)</code> to store the loss for each evaluation iteration.</li>\n",
    "    <li>Loops over <code>eval_iters</code> number of iterations and retrieves a batch of data for the given split using <code>get_batch(split)</code>.</li>\n",
    "    <li>Passes the input data <code>X</code> and target <code>Y</code> through the PyTorch model <code>model</code> to get the logits and the loss for the batch.</li>\n",
    "    <li>Stores the loss value as a scalar in the <code>losses</code> tensor at the index <code>k</code>.</li>\n",
    "    <li>Computes the mean of the <code>losses</code> tensor and stores the result in the <code>out</code> dictionary with the key being the split.</li>\n",
    "    <li>Sets the model back to training mode using <code>model.train()</code>.</li>\n",
    "    <li>Returns the <code>out</code> dictionary containing the average loss for each dataset.</li>\n",
    "</ol>\n",
    "<p><em>Note: It is assumed that the variables <code>model</code>, <code>eval_iters</code>, and <code>get_batch()</code> are defined and accessible within the scope of this function.</em></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "47206960",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad() # tells pytorch we will not call \"backward\" on the function (back propagation)\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cadef73-b77a-40dd-8d8f-f7ed7f5a2ab7",
   "metadata": {},
   "source": [
    "<h4> D. class Head </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1203cf-7cd3-4a15-8680-89d488c832ce",
   "metadata": {},
   "source": [
    "<strong><code>class Head(nn.Module):</code></strong>\n",
    "\n",
    "<p>\n",
    "In natural language processing, self-attention is a technique used in deep learning models to help the model understand the context of a sentence or a document. It works by assigning weights to each word in the input text, based on the relevance of that word to the other words in the text. These weights help the model to focus on the most important words and ignore the less important ones.\n",
    "A head of self-attention is a single part of the self-attention mechanism. Think of it like a small flashlight that helps the model to focus on a specific part of the input text. Each head of self-attention works independently to assign weights to the words in the input text, and then combines the weighted representation of the input text to create a more accurate understanding of the context.\n",
    "\n",
    "Just like how you might use multiple flashlights to illuminate different parts of a room, deep learning models can use multiple heads of self-attention to better understand different aspects of the input text. By combining the information from multiple heads of self-attention, the model can create a more complete and accurate representation of the input text, which helps it perform better on natural language processing tasks like language translation and text classification.</p>\n",
    "<strong><code>Parameters:</code></strong>\n",
    "\n",
    "<ul>\n",
    "<li><strong><code>head_size (int)</code></strong>: The size of the subspace for key, query, and value tensors.</li>\n",
    "</ul>\n",
    "<strong><code>Attributes:</code></strong>\n",
    "<ul>\n",
    "<li><strong><code>key (nn.Linear)</code></strong>: Linear layer for projecting the input tensor into the key subspace.</li>\n",
    "<li><strong><code>query (nn.Linear)</code></strong>: Linear layer for projecting the input tensor into the query subspace.</li>\n",
    "<li><strong><code>value (nn.Linear)</code></strong>: Linear layer for projecting the input tensor into the value subspace.</li>\n",
    "<li><strong><code>tril (torch.Tensor)</code></strong>: Lower-triangular matrix of ones, with the same size as the sequence length, used to mask out attention scores corresponding to positions that have not been seen yet during training.</li>\n",
    "<li><strong><code>dropout (nn.Dropout)</code></strong>: Dropout layer to prevent overfitting.</li>\n",
    "</ul>\n",
    "<strong><code>Inputs:</code></strong>\n",
    "<ul>\n",
    "<li><strong><code>x (torch.Tensor)</code></strong>: Input tensor of shape `(B, T, C)`, where `B` is the batch size, `T` is the sequence length, and `C` is the hidden size of the input tensor.</li>\n",
    "</ul>\n",
    "<strong><code>Outputs:</code></strong>\n",
    "<ul>\n",
    "<li><strong><code>out (torch.Tensor)</code></strong>: Output tensor of the same shape as the input tensor.</li>\n",
    "</ul>\n",
    "<strong><code>Methods:</code></strong>\n",
    "<ul>\n",
    "<li><strong><code>forward(x: torch.Tensor) -&gt; torch.Tensor:</code></strong> Computes the self-attention of the input tensor `x`, and returns the output tensor.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d5478053-ec37-4c89-a8db-19b24053478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model copied from https://github.com/karpathy/nanoGPT\n",
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9810671-57a8-4413-a67b-0aa96d412cab",
   "metadata": {},
   "source": [
    "<h4> E. class MultiHeadAttention "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2d9175-1374-4f3b-9952-9d3b72180e92",
   "metadata": {},
   "source": [
    "<div>\n",
    "<strong><code>class MultiHeadAttention(nn.Module):</code></strong>\n",
    "<p>The MultiHeadAttention module is an implementation of the multi-head attention mechanism used in transformer models. The idea behind multi-head attention is to split the input tensor into multiple \"heads\", and apply the self-attention mechanism to each head independently. This allows the model to attend to different parts of the input tensor with different sets of weights, which can help the model learn more diverse and meaningful representations.\n",
    "The MultiHeadAttention module takes an input tensor x of shape (B, T, C), where B is the batch size, T is the sequence length, and C is the hidden size of the input tensor. The num_heads parameter specifies the number of attention heads, and the head_size parameter specifies the size of each attention head.\n",
    "\n",
    "In the init method, the MultiHeadAttention module creates a list of num_heads Head modules, each with a size of head_size. Each Head module computes the attention scores and corresponding weighted values for a single attention head. The MultiHeadAttention module then applies a linear projection to the concatenated output of the attention heads to transform it back into the original hidden size C. Finally, a dropout layer is applied to prevent overfitting.\n",
    "\n",
    "In the forward method, the MultiHeadAttention module passes the input tensor x through each Head module, concatenates the outputs along the last dimension, and applies the linear projection and dropout layers to the concatenated output. The resulting tensor has the same shape as the input tensor.\n",
    "\n",
    "Overall, the MultiHeadAttention module allows the transformer model to attend to multiple parts of the input tensor at once, and learn more complex relationships between different parts of the input. This can lead to better performance on tasks that require modeling complex relationships between different parts of the input, such as natural language understanding or image processing.</p>\n",
    "<strong><code>Parameters:</code></strong>\n",
    "\n",
    "<ul>\n",
    "<li><strong><code>num_heads (int)</code></strong>: The number of heads in the multi-head attention.</li>\n",
    "<li><strong><code>head_size (int)</code></strong>: The size of each head in the multi-head attention.</li>\n",
    "</ul>\n",
    "<strong><code>Attributes:</code></strong>\n",
    "<ul>\n",
    "<li><strong><code>heads (nn.ModuleList)</code></strong>: Module list containing the individual heads of self-attention.</li>\n",
    "<li><strong><code>proj (nn.Linear)</code></strong>: Linear layer for projecting the concatenated output of the individual attention heads into the original hidden size.</li>\n",
    "<li><strong><code>dropout (nn.Dropout)</code></strong>: Dropout layer to prevent overfitting.</li>\n",
    "</ul>\n",
    "<strong><code>Inputs:</code></strong>\n",
    "<ul>\n",
    "<li><strong><code>x (torch.Tensor)</code></strong>: Input tensor of shape `(B, T, C)`, where `B` is the batch size, `T` is the sequence length, and `C` is the hidden size of the input tensor.</li>\n",
    "</ul>\n",
    "<strong><code>Outputs:</code></strong>\n",
    "<ul>\n",
    "<li><strong><code>out (torch.Tensor)</code></strong>: Output tensor of the same shape as the input tensor.</li>\n",
    "</ul>\n",
    "<strong><code>Methods:</code></strong>\n",
    "<ul>\n",
    "<li><strong><code>forward(x: torch.Tensor) -&gt; torch.Tensor:</code></strong> Computes the multi-head self-attention of the input tensor `x`, and returns the output tensor.</li>\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bd134baf-56b5-4649-837d-e784b4d813fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model copied from https://github.com/karpathy/nanoGPT\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4190a71f-3a9f-43fd-9152-d4dc79c10f2e",
   "metadata": {},
   "source": [
    "<h4> F. class FeedForward </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e663690-c8cf-4d19-beba-e636d881f20c",
   "metadata": {},
   "source": [
    "<div>\n",
    "<strong><code>class FeedFoward(nn.Module):</code></strong>\n",
    "<p>The FeedFoward module is a two-layer feedforward neural network used in transformer models. It consists of a fully connected layer followed by a ReLU activation function, another fully connected layer, and a dropout layer. The module takes an input tensor x of shape (B, T, C), where B is the batch size, T is the sequence length, and C is the hidden size of the input tensor.\n",
    "In the <code>init</code> method, the FeedFoward module creates a sequential neural network with two fully connected layers. The first layer has an output size of 4 * n_embd and uses the ReLU activation function. The second layer has an output size of n_embd. Finally, a dropout layer is applied to prevent overfitting.\n",
    "\n",
    "In the <code>forward</code> method, the FeedFoward module passes the input tensor x through the sequential neural network to compute the output tensor. The resulting tensor has the same shape as the input tensor.\n",
    "\n",
    "Overall, the FeedFoward module is used to add non-linearity and increase the expressiveness of the transformer model. The two-layer feedforward neural network can learn complex representations of the input tensor, which can be helpful for tasks that require modeling complex relationships between different parts of the input, such as natural language understanding or image processing.</p>\n",
    "<strong><code>Parameters:</code></strong>\n",
    "\n",
    "<ul>\n",
    "<li><strong><code>n_embd (int)</code></strong>: The size of the input and output tensors of the feedforward neural network.</li>\n",
    "</ul>\n",
    "<strong><code>Attributes:</code></strong>\n",
    "<ul>\n",
    "<li><strong><code>net (nn.Sequential)</code></strong>: Sequential neural network containing the fully connected layers and dropout layer of the feedforward neural network.</li>\n",
    "</ul>\n",
    "<strong><code>Inputs:</code></strong>\n",
    "<ul>\n",
    "<li><strong><code>x (torch.Tensor)</code></strong>: Input tensor of shape `(B, T, C)`, where `B` is the batch size, `T` is the sequence length, and `C` is the hidden size of the input tensor.</li>\n",
    "</ul>\n",
    "<strong><code>Outputs:</code></strong>\n",
    "<ul>\n",
    "<li><strong><code>out (torch.Tensor)</code></strong>: Output tensor of the same shape as the input tensor.</li>\n",
    "</ul>\n",
    "<strong><code>Methods:</code></strong>\n",
    "<ul>\n",
    "<li><strong><code>forward(x: torch.Tensor) -&gt; torch.Tensor:</code></strong> Computes the feedforward neural network of the input tensor `x`, and returns the output tensor.</li>\n",
    "</ul>\n",
    "<p>Overall, the <strong><code>FeedFoward</code></strong> module is used to add non-linearity and increase the expressiveness of the transformer model. The two-layer feedforward neural network can learn complex representations of the input tensor, which can be helpful for tasks that require modeling complex relationships between different parts of the input, such as natural language understanding or image processing.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "353cc8cd-ac0e-4295-b060-43bd1d2d08ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedFoward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc619bc-dbfd-4cd5-a79e-309d6e44a9fb",
   "metadata": {},
   "source": [
    "<h4> G. class Block </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31655804-c0f6-4d39-b113-3cb0164c8d67",
   "metadata": {},
   "source": [
    "<div>\n",
    "<strong><code>class Block(nn.Module):</code></strong>\n",
    "<p>The Block module is a building block of the transformer model that consists of a multi-head self-attention layer, a feedforward neural network layer, and two layer normalization layers. It takes an input tensor x of shape (B, T, C), where B is the batch size, T is the sequence length, and C is the hidden size of the input tensor.\n",
    "In the <code>init</code> method, the Block module initializes a MultiHeadAttention layer and a FeedFoward layer with the specified embedding dimension <code>n_embd</code> and number of attention heads <code>n_head</code>. It also initializes two LayerNorm layers, which are used to normalize the output of the self-attention and feedforward layers, respectively.\n",
    "\n",
    "In the <code>forward</code> method, the Block module applies the self-attention layer to the input tensor, adds the output of the self-attention layer to the original input tensor using residual connections, normalizes the resulting tensor using layer normalization, applies the feedforward layer to the normalized tensor, adds the output of the feedforward layer to the previous tensor using residual connections, normalizes the resulting tensor using layer normalization, and returns the final tensor.\n",
    "\n",
    "Overall, the Block module is a key building block of the transformer model that allows the model to process sequences of variable length and capture complex relationships between different parts of the input. By stacking multiple Block modules together, the transformer model can learn complex representations of the input sequence that can be used for a wide range of natural language processing tasks, such as machine translation, text classification, and text generation.</p>\n",
    "<strong><code>Parameters:</code></strong>\n",
    "\n",
    "<ul>\n",
    "<li><strong><code>n_embd (int)</code></strong>: The embedding dimension of the input tensor.</li>\n",
    "<li><strong><code>n_head (int)</code></strong>: The number of attention heads to use in the multi-head self-attention layer.</li>\n",
    "</ul>\n",
    "<strong><code>Attributes:</code></strong>\n",
    "<ul>\n",
    "<li><strong><code>sa (MultiHeadAttention)</code></strong>: Multi-head self-attention layer that takes the input tensor as input and returns the output tensor after applying the self-attention mechanism.</li>\n",
    "<li><strong><code>ffwd (FeedFoward)</code></strong>: Feedforward neural network layer that takes the input tensor as input and returns the output tensor after applying a two-layer feedforward neural network.</li>\n",
    "<li><strong><code>ln1 (nn.LayerNorm)</code></strong>: First layer normalization layer that takes the output tensor of the self-attention layer as input and returns the normalized output tensor.</li>\n",
    "<li><strong><code>ln2 (nn.LayerNorm)</code></strong>: Second layer normalization layer that takes the output tensor of the feedforward neural network layer as input and returns the normalized output tensor.</li>\n",
    "</ul>\n",
    "<strong><code>Inputs:</code></strong>\n",
    "<ul>\n",
    "<li><strong><code>x (torch.Tensor)</code></strong>: Input tensor of shape <code>(B, T, C)</code>, where <code>B</code> is the batch size, <code>T</code> is the sequence length, and <code>C</code> is the hidden size of the input tensor.</li>\n",
    "</ul>\n",
    "<strong><code>Outputs:</code></strong>\n",
    "<ul>\n",
    "<li><strong><code>x (torch.Tensor)</code></strong>: Output tensor of the same shape as the input tensor.</li>\n",
    "</ul>\n",
    "<strong><code>Methods:</code></strong>\n",
    "<ul>\n",
    "<li><code>forward(x: torch.Tensor) -&gt; torch.Tensor:</code> Computes the output tensor by passing the input tensor through the multi-head self-attention layer, adding the resulting tensor to the input tensor using residual connections, normalizing the resulting tensor using layer normalization, passing the normalized tensor through the feedforward neural network layer, adding the resulting tensor to the previous tensor using residual connections, normalizing the resulting tensor using layer normalization, and returning the final tensor.</li>\n",
    "</ul>\n",
    "<p>Overall, the `Block` module is a key building block of the transformer model that allows the model to process sequences of variable length and capture complex relationships between different parts of the input. By stacking multiple `Block` modules together, the transformer model can learn complex representations of the input sequence that can be used for a wide range of natural language processing tasks, such as machine translation, text classification, and text generation.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d5f8140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79be983d-14f8-4cf9-b8c2-b65136a89eb0",
   "metadata": {},
   "source": [
    "<h4> Function: BigramLanguageModel </h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7592e-0b46-4f8e-8e87-6c1d212052d1",
   "metadata": {},
   "source": [
    "<strong>Function: BigramLanguageModel</strong><br>\n",
    "This function defines a simple bigram language model using PyTorch. It consists of an embedding layer, a positional encoding layer, a number of transformer blocks, a final layer normalization, and a linear projection layer to generate logits for the next token. It can also generate new tokens by sampling from the distribution of the next token.\n",
    "\n",
    "<strong>Parameters:</strong><br>\n",
    "None.\n",
    "\n",
    "<strong>Attributes:</strong><br>\n",
    "\n",
    "<ul>\n",
    "<li><code>token_embedding_table (nn.Embedding)</code>: Lookup table to read off the logits for the next token from each token.</li>\n",
    "<li><code>position_embedding_table (nn.Embedding)</code>: Lookup table to encode the positional information of each token.</li>\n",
    "<li><code>blocks (nn.Sequential)</code>: Sequential container consisting of a number of transformer blocks.</li>\n",
    "<li><code>ln_f (nn.LayerNorm)</code>: Layer normalization layer for the final output.</li>\n",
    "<li><code>lm_head (nn.Linear)</code>: Linear projection layer to generate logits for the next token.</li>\n",
    "</ul>\n",
    "<strong>Inputs:</strong><br>\n",
    "\n",
    "<ul>\n",
    "<li><code>idx (torch.Tensor)</code>: (B,T) tensor of integers representing the token indices.</li>\n",
    "<li><code>targets (torch.Tensor)</code>: (B,T) tensor of integers representing the target token indices. If None, the function returns the logits without computing the loss.</li>\n",
    "</ul>\n",
    "<strong>Outputs:</strong><br>\n",
    "\n",
    "<ul>\n",
    "<li><code>logits (torch.Tensor)</code>: (B,T,vocab_size) tensor of logits for the next token.</li>\n",
    "<li><code>loss (torch.Tensor)</code>: (1,) tensor of the cross-entropy loss between the predicted logits and the target indices. If targets is None, loss is None.</li>\n",
    "</ul>\n",
    "<strong>Methods:</strong><br>\n",
    "\n",
    "<ul>\n",
    "<li><code>forward(idx: torch.Tensor, targets: torch.Tensor = None) -&gt; torch.Tensor:</code> Computes the logits for the next token given the input tensor idx, and returns the logits and the cross-entropy loss between the predicted logits and the target indices.</li>\n",
    "<li><code>generate(idx: torch.Tensor, max_new_tokens: int) -&gt; torch.Tensor:</code> Generates new tokens by sampling from the distribution of the next token given the input tensor idx, and returns the sequence of generated tokens.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f35bd21a-9a89-41b8-9531-ea98e35d69cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fc69cfeb-1ef8-4fd9-8eef-1495b7063704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259.082257 M parameters\n"
     ]
    }
   ],
   "source": [
    "model = BigramLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2c8b18",
   "metadata": {},
   "source": [
    "<h3> 8. Train Bigram Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dc04f05d-98c0-48ac-8aea-281b590c6466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pytz\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_model(filename, model, max_iters, eval_interval, plateau, patience, learning_rate):\n",
    "    # Check if the trained model already exists\n",
    "    if os.path.exists(filename):\n",
    "        use_existing_model = input('Trained model already exists. Do you want to use it? (y/n) ')\n",
    "        if use_existing_model.lower() == 'y':\n",
    "            # Load the existing trained model\n",
    "            model.load_state_dict(torch.load(filename))\n",
    "            return model\n",
    "    \n",
    "    # Train the model\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Create a list to store the losses\n",
    "    loss_history = []\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    num_epochs_no_improvement = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    start_datetime = datetime.datetime.now(pytz.timezone('US/Pacific'))\n",
    "\n",
    "    for iter in range(max_iters):\n",
    "\n",
    "        # sample a batch of data\n",
    "        xb, yb = get_batch('train')\n",
    "\n",
    "        # evaluate the loss\n",
    "        logits, loss = model(xb, yb)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # every once in a while evaluate the loss on train and val sets\n",
    "        if iter % eval_interval == 0:\n",
    "            losses = estimate_loss()\n",
    "            current_time = time.time()\n",
    "            elapsed_time = current_time - start_time\n",
    "            current_datetime = datetime.datetime.now(pytz.timezone('US/Pacific'))\n",
    "            time_str = current_datetime.strftime('%Y-%m-%d %H:%M:%S %Z%z')\n",
    "\n",
    "            # Store the loss every eval_interval iterations\n",
    "            loss_history.append({'train': losses['train'], 'val': losses['val'], 'step': iter,\n",
    "                                 'timestamp': current_datetime, 'elapsed_time': elapsed_time})\n",
    "\n",
    "            # Check for plateau\n",
    "            if best_val_loss - losses['val'] < plateau:\n",
    "                num_epochs_no_improvement += 1\n",
    "                if num_epochs_no_improvement >= patience:\n",
    "                    print(f\"Validation loss plateaued for {patience} epochs, stopping early...\")\n",
    "                    break\n",
    "            else:\n",
    "                best_val_loss = losses['val']\n",
    "                num_epochs_no_improvement = 0\n",
    "\n",
    "            if iter > 2 * eval_interval:\n",
    "                time_elapsed = current_datetime - start_datetime\n",
    "                time_per_iter = time_elapsed / iter\n",
    "                est_remaining_iters = max_iters - iter\n",
    "                est_remaining_time = est_remaining_iters * time_per_iter\n",
    "                est_completion_time = (current_datetime + est_remaining_time).strftime('%Y-%m-%d %H:%M:%S %Z%z')\n",
    "                print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, timestamp {time_str}, elapsed time {elapsed_time:.4f}s, est. completion {est_completion_time}\")\n",
    "            else:\n",
    "                print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, timestamp {time_str}, elapsed time {elapsed_time:.4f}s\")\n",
    "\n",
    "        if iter == max_iters-1:\n",
    "            print(f\"Training completed: elapsed time {elapsed_time:.4f}s\")\n",
    "\n",
    "    # Plot the loss history\n",
    "    train_losses = [l['train'] for l in loss_history]\n",
    "    val_losses = [l['val'] for l in loss_history]\n",
    "    steps = [l['step'] for l in loss_history]\n",
    "    plt.plot(steps, train_losses, label='Train Loss')\n",
    "    plt.plot(steps, val_losses, label='Validation Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train and Validation Loss over Time')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Save the trained model\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f95ca-3513-4780-86fd-20c3942d7320",
   "metadata": {},
   "source": [
    "<b> Look for Existing Trained Model </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0f115e6f-c61f-459d-82b2-54d93e9e7e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lincoln_nn_llm.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "if os.path.isfile('lincoln_nn_llm.pt'):\n",
    "    filename = 'lincoln_nn_llm.pt'\n",
    "elif os.path.isfile('models/lincoln_nn_llm.pt'):\n",
    "    filename = 'models/lincoln_nn_llm.pt'\n",
    "else:\n",
    "    filename = 'lincoln_nn_llm.pt'\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d954d53-27d0-4cbf-bfe7-5a7bbbc11796",
   "metadata": {},
   "source": [
    "<b> Launch Training Sequence </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c6c9253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Trained model already exists. Do you want to use it? (y/n)  n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 9.3238, val loss 9.2370, timestamp 2023-05-05 16:14:21 PDT-0700, elapsed time 146.2669s\n",
      "step 50: train loss 6.0900, val loss 6.0343, timestamp 2023-05-05 16:17:46 PDT-0700, elapsed time 351.2972s\n",
      "step 100: train loss 5.3434, val loss 5.2921, timestamp 2023-05-05 16:21:12 PDT-0700, elapsed time 556.8737s\n",
      "step 150: train loss 4.9868, val loss 4.9588, timestamp 2023-05-05 16:24:38 PDT-0700, elapsed time 762.5300s, est. completion 2023-05-05 20:26:06 PDT-0700\n",
      "step 200: train loss 4.7015, val loss 4.7082, timestamp 2023-05-05 16:28:03 PDT-0700, elapsed time 968.2853s, est. completion 2023-05-05 20:13:59 PDT-0700\n",
      "step 250: train loss 4.5011, val loss 4.5332, timestamp 2023-05-05 16:31:29 PDT-0700, elapsed time 1174.0621s, est. completion 2023-05-05 20:06:44 PDT-0700\n",
      "step 300: train loss 4.3259, val loss 4.4282, timestamp 2023-05-05 16:34:55 PDT-0700, elapsed time 1379.9377s, est. completion 2023-05-05 20:01:54 PDT-0700\n",
      "step 350: train loss 4.1842, val loss 4.3175, timestamp 2023-05-05 16:38:21 PDT-0700, elapsed time 1585.6124s, est. completion 2023-05-05 19:58:26 PDT-0700\n",
      "step 400: train loss 4.0456, val loss 4.2369, timestamp 2023-05-05 16:41:46 PDT-0700, elapsed time 1791.4281s, est. completion 2023-05-05 19:55:51 PDT-0700\n",
      "step 450: train loss 3.9447, val loss 4.1737, timestamp 2023-05-05 16:45:12 PDT-0700, elapsed time 1997.1791s, est. completion 2023-05-05 19:53:50 PDT-0700\n",
      "step 500: train loss 3.8389, val loss 4.1248, timestamp 2023-05-05 16:48:38 PDT-0700, elapsed time 2202.9767s, est. completion 2023-05-05 19:52:13 PDT-0700\n",
      "step 550: train loss 3.7479, val loss 4.0701, timestamp 2023-05-05 16:52:04 PDT-0700, elapsed time 2408.9109s, est. completion 2023-05-05 19:50:55 PDT-0700\n",
      "step 600: train loss 3.6339, val loss 4.0417, timestamp 2023-05-05 16:55:30 PDT-0700, elapsed time 2614.6248s, est. completion 2023-05-05 19:49:48 PDT-0700\n",
      "step 650: train loss 3.5427, val loss 4.0056, timestamp 2023-05-05 16:58:55 PDT-0700, elapsed time 2820.1209s, est. completion 2023-05-05 19:48:51 PDT-0700\n",
      "step 700: train loss 3.4636, val loss 3.9864, timestamp 2023-05-05 17:02:21 PDT-0700, elapsed time 3025.7802s, est. completion 2023-05-05 19:48:03 PDT-0700\n",
      "step 750: train loss 3.3683, val loss 3.9679, timestamp 2023-05-05 17:05:46 PDT-0700, elapsed time 3231.4288s, est. completion 2023-05-05 19:47:21 PDT-0700\n",
      "step 800: train loss 3.3005, val loss 3.9552, timestamp 2023-05-05 17:09:12 PDT-0700, elapsed time 3437.0459s, est. completion 2023-05-05 19:46:44 PDT-0700\n",
      "step 850: train loss 3.2118, val loss 3.9420, timestamp 2023-05-05 17:12:38 PDT-0700, elapsed time 3642.5619s, est. completion 2023-05-05 19:46:11 PDT-0700\n",
      "step 900: train loss 3.1438, val loss 3.9351, timestamp 2023-05-05 17:16:03 PDT-0700, elapsed time 3848.2348s, est. completion 2023-05-05 19:45:43 PDT-0700\n",
      "step 950: train loss 3.0544, val loss 3.9209, timestamp 2023-05-05 17:19:29 PDT-0700, elapsed time 4053.7331s, est. completion 2023-05-05 19:45:16 PDT-0700\n",
      "step 1000: train loss 2.9874, val loss 3.9469, timestamp 2023-05-05 17:22:54 PDT-0700, elapsed time 4259.2692s, est. completion 2023-05-05 19:44:53 PDT-0700\n",
      "step 1050: train loss 2.9015, val loss 3.9484, timestamp 2023-05-05 17:26:20 PDT-0700, elapsed time 4464.9231s, est. completion 2023-05-05 19:44:32 PDT-0700\n",
      "Validation loss plateaued for 3 epochs, stopping early...\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "trained_model = train_model(filename, model, max_iters, eval_interval, plateau, patience, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1d83289d-1aed-4c35-9259-267e9f18f5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal. We well spoken of even Territories and Fillmore men, where all of us not to be recognizing us. [Mr. Lincoln read ``three cheers to make War of the reporter and answer around,'' ``abilities,'' ``squelched out,''---which he may be an amendment, and inserting ``oubt'' of the ``No clear articles.'' At the State of this, he admitted at least when he declares that is the first body of the National Bank, the community will be justified in the House of Representatives, and which were of the power oversl---that is to be spread upon that resolution. He insisted that, and therefore, nothing else shall be the exposition of the removal under that operation or by a commission of signers, applied  at the least. That Prince's strains, by what was founded in the House of Representatives, at pages It happens and on public Journals, that many were as strongly opposed to the governmentics in it. In a complete execution I suppose that the resolution were left with me was marked ``uniary therein'' was exercised between the late collection and cause of the known revenue from the part of the records of the recent session of Congress, containing some professional making slavery question. And then I am glad if the article is a proposition in the House of the Union and the States in which our correspondence between this. I am wrong, and its affairs may be in the use of applying the spread and perpetuation and evils to the necessity. And I may have myself that upon any occasion to combine in addition to our keepingou from Europe behind those things at the present session of Congress. Please show you, and inform---go on paper. Send him long letters , and for President. If it be getting  to any one else, and yet other Yordignment of our men, you are evidently now. Are not know that any of the cotton was chosen by once a resolution[re]m? except a matter that portion of the work is to come to my neighboruff; that even if it were sufficient for the public Treasury to his proclamations, was in conflict with him, and I would, and I hope it does not appear for all, I think if it will be safe. Copy otherwise, it is shown that Hon. Hay, I use commonhome, has been directly, inroads on his military commission. I certainly certainly now and was so distinctly blocked up. I shall detain it in the time for the time the gentlemen, I\n"
     ]
    }
   ],
   "source": [
    "# generate from the model\n",
    "prompt = \"Four score and seven years ago our fathers brought forth on this continent, a new nation, conceived in Liberty, and dedicated to the proposition that all men are created equal.\"\n",
    "context = torch.tensor(gpt2enc.encode(prompt), dtype=torch.long, device=device).unsqueeze(0) # add batch dimension\n",
    "generated_tokens = trained_model.generate(context, max_new_tokens=500)[0].tolist()\n",
    "generated_text = gpt2enc.decode(generated_tokens)\n",
    "print(generated_text)\n",
    "\n",
    "# Output the generated text to a file\n",
    "with open('lincoln_nn_llm_text.txt', 'w') as f:\n",
    "    f.write(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b600dee-1a54-4068-8a08-a3366850fd59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
